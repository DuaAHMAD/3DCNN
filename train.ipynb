{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# import argparse\n",
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils import data as Data\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from learning.datasets import BDIDerivedDataset\n",
    "from learning.models import BDI_3D_Conv, BDI_3D_Conv_Simple\n",
    "from learning.loops import train, validate, test\n",
    "import config\n",
    "import learning.dataset_config as learning_config\n",
    "# from tensorboard_logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python train.py --datasets BDI_DERIVED --output  --batch 50 --epochs 20 --lr 1e-4^C\n",
    "\n",
    "\n",
    "dataroot= config.ALIGNED_FACES_FOLDER\n",
    "datasets= \"BDI_DERIVED\"\n",
    "workers= 4\n",
    "batch= 75\n",
    "epochs= 20\n",
    "cpu= False\n",
    "# seed= 1\n",
    "seed = random.randint(1, 10000)\n",
    "seed\n",
    "checkpoint='None'\n",
    "output= '/timo/datasets/Dua/3Dcnn/output'\n",
    "print_freq= 10\n",
    "visualize=False\n",
    "lr=1e-4\n",
    "submedian= False\n",
    "overwrite=False\n",
    "augment= False\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='BDI Training Script')\n",
    "# parser.add_argument('--dataroot', default=config.ALIGNED_FACES_FOLDER, type=str, help='path to dataset')\n",
    "# parser.add_argument('--datasets', type=str, help='datasets used for training and validation')\n",
    "# parser.add_argument('--workers', '-j', default=4, type=int, help='number of data loading workers')\n",
    "# parser.add_argument('--batch', type=int, default=75, help='input batch size')\n",
    "# parser.add_argument('--epochs', default=25, type=int, help='number of epochs to run')\n",
    "# parser.add_argument('--cpu', action='store_true', help='run without cuda')\n",
    "# parser.add_argument('--seed', type=int, help='manual seed')\n",
    "# parser.add_argument('--checkpoint', type=str, help='location of the checkpoint to load')\n",
    "# parser.add_argument('--output', default='/home/mohammad/output/emotion', type=str,\n",
    "#                     help='folder to output model checkpoints')\n",
    "\n",
    "# parser.add_argument('--print-freq', default=10, type=int, help='print frequency')\n",
    "# parser.add_argument('--visualize', action='store_true', help='evaluate model on validation set')\n",
    "# parser.add_argument('--lr', default=1e-3, type=float, help='learning rate')\n",
    "# parser.add_argument('--submedian', action='store_true', help='run on natural data')\n",
    "# parser.add_argument('--overwrite', action='store_true', help='overwrite the prediction data or not')\n",
    "# parser.add_argument('--augment', action='store_true', help='do data augmentation or not')\n",
    "\n",
    "#parser.set_defaults(augment=True)\n",
    "\n",
    "# # parse arguments\n",
    "# args = parser.parse_args()\n",
    "# if args.seed is None:\n",
    "#     args.seed = random.randint(1, 10000)\n",
    "\n",
    "if visualize:\n",
    "    # evaluation model\n",
    "    output = checkpoint[0:-4]\n",
    "\n",
    "# # print arguments\n",
    "# print(\"Summary of Arguments:\")\n",
    "# for key, val in vars(args).items():\n",
    "#     print(\"{:10} {}\".format(key, val))\n",
    "\n",
    "\n",
    "# handle random seed\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(int(seed))\n",
    "if not cpu:\n",
    "    torch.cuda.manual_seed_all(int(seed))\n",
    "\n",
    "# create output folder\n",
    "if os.path.exists(output):\n",
    "    if not visualize and checkpoint == None:\n",
    "        if len([f for f in os.listdir(output) if \"model\" in f]) > 0:\n",
    "            raise(RuntimeError(\"Output folder {} already exist.\".format(output)))\n",
    "        else:\n",
    "            shutil.rmtree(output)\n",
    "            os.makedirs(output)\n",
    "else:\n",
    "    os.makedirs(output)\n",
    "\n",
    "# create tensorboard, args and code copy\n",
    "# if not visualize:\n",
    "# #     Logger_train = Logger(logdir=join(args.output, \"train\"), flush_secs=2)\n",
    "# #     Logger_val = Logger(logdir=join(args.output, \"val\"), flush_secs=2)\n",
    "#     f_log = open(os.path.join(output, 'args.txt'), 'w')\n",
    "#     for key, val in vars(args).items():\n",
    "#         print(\"{} {}\".format(key, val), file=f_log)\n",
    "#     f_log.close()\n",
    "#     if checkpoint is None:\n",
    "#         shutil.copytree('.', os.path.join(output, 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify train and validate data folders\n",
    "\n",
    "train_exp, validate_exp = learning_config.get_train_val_folders(name=datasets)\n",
    "\n",
    "# #TODO\n",
    "# if args.visualize:\n",
    "#     validate_exp += train_exp[0:2]\n",
    "#     validate_exp += [\"exp0718-2-01\", \"exp0718-2-02\", \"exp0719-2-01\", \"exp0719-2-02\"]\n",
    "\n",
    "# build datasets\n",
    "train_dataset = BDIDerivedDataset(\n",
    "    folders=[dataroot.format(exp=exp) for exp in train_exp],\n",
    "    submedian=submedian,\n",
    "    flip=augment\n",
    ")\n",
    "validate_dataset = BDIDerivedDataset(\n",
    "    folders=[dataroot.format(exp=exp) for exp in validate_exp],\n",
    "    submedian=submedian,\n",
    "    flip=False,\n",
    "    return_idx=visualize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data loaders\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=True,\n",
    "    num_workers=workers,\n",
    "    pin_memory=False\n",
    ")\n",
    "validate_loader = Data.DataLoader(\n",
    "    dataset=validate_dataset,\n",
    "    batch_size=batch,\n",
    "    shuffle=False,\n",
    "    num_workers=workers,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# print(len(train_loader))\n",
    "# print(len(validate_loader))\n",
    "#    \n",
    "# for idx, data in enumerate(train_loader):\n",
    "#     (rf, labels) = data\n",
    "# for idx, data in enumerate(validate_loader):\n",
    "#     (rf, labels) = data\n",
    "# #     print(labels)\n",
    "# #     print(np.array(rf).shape)\n",
    "# exit()\n",
    "\n",
    "\n",
    "# build model and optimizer\n",
    "model = BDI_3D_Conv_Simple()\n",
    "# model = BDI_3D_Conv()\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if not cpu else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print('# of params:', str(sum([p.numel() for p in model.parameters()])))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# optionally load model from a checkpoint\n",
    "if checkpoint:\n",
    "    if os.path.isfile(checkpoint):\n",
    "        model = torch.load(checkpoint)\n",
    "        if not visualize:\n",
    "            start_epoch = int(checkpoint.split('.')[-2].split('_')[-1]) + 1\n",
    "    else:\n",
    "        raise(RuntimeError(\"no checkpoint found at '{}'\".format(checkpoint)))\n",
    "else:\n",
    "    start_epoch = 1\n",
    "\n",
    "lr = lr\n",
    "\n",
    "if visualize:\n",
    "    # evaluation model\n",
    "    if not checkpoint:\n",
    "        raise(RuntimeWarning(\"visualizing a random initialized model\"))\n",
    "    if overwrite:\n",
    "        shutil.rmtree(output)\n",
    "    test(model, validate_loader, 0, args, criterion=criterion)\n",
    "else:\n",
    "    # train and validate loop\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "        train_loss = train(model, train_loader, optimizer, epoch, args, criterion=criterion)\n",
    "        val_loss = validate(model, validate_loader, epoch, args, criterion=criterion)\n",
    "\n",
    "        # do tensorboard and checkpointing:\n",
    "#         Logger_train.log_value('loss', train_loss, epoch)\n",
    "#         Logger_val.log_value('loss', val_loss[0], epoch)\n",
    "#         Logger_val.log_value('accuracy', val_loss[1], epoch)\n",
    "#         Logger_train.log_value('lr', lr, epoch)\n",
    "        torch.save(model, '{}/model_epoch_{}.pth'.format(output, epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
